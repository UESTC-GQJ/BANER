{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 加载 JSONL 文件\n",
    "dataset = load_dataset('json', data_files='intra/train_10_5.jsonl')\n",
    "\n",
    "print(dataset)\n",
    "# 获取第一个数据集\n",
    "first_record = dataset['train'][0]\n",
    "\n",
    "# 打印第一条内容\n",
    "print(first_record)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def extract_entities(words, labels):\n",
    "    \"\"\"\n",
    "    根据标签列表中的非 'O' 元素提取对应的实体和标签。\n",
    "\n",
    "    :param words: 单词列表\n",
    "    :param labels: 标签列表\n",
    "    :return: 包含实体和其标签的元组列表\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    current_label = None\n",
    "\n",
    "    for word, label in zip(words, labels):\n",
    "        if label == 'O':\n",
    "            if current_entity:\n",
    "                entities.append((' '.join(current_entity), current_label))\n",
    "                current_entity = []\n",
    "                current_label = None\n",
    "        else:\n",
    "            if current_label == label:\n",
    "                current_entity.append(word)\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities.append((' '.join(current_entity), current_label))\n",
    "                current_entity = [word]\n",
    "                current_label = label\n",
    "\n",
    "    if current_entity:\n",
    "        entities.append((' '.join(current_entity), current_label))\n",
    "\n",
    "    return entities\n",
    "# 加载JSONL文件\n",
    "with open('episode-data/intra/test_5_1.jsonl', 'r') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "# 提取support中的word和label\n",
    "support_words = []\n",
    "support_entity = []\n",
    "support_labels = []\n",
    "zidian={}\n",
    "zidian_list=[]\n",
    "for entry in data:\n",
    "    word_lists = entry['support']['word']\n",
    "    label_lists = entry['support']['label']\n",
    "    for idx in range(len(word_lists)):\n",
    "        result = extract_entities(word_lists[idx], label_lists[idx])\n",
    "    # 将每个word列表合并成一个字符串\n",
    "        combined_words = ' '.join(word_lists[idx])\n",
    "    # support_words.append(combined_words)\n",
    "    zidian[\"instruction\"]=\"Please extract the entity of \"+result[0][1]+\" in the input sentence given below.\"\n",
    "    zidian[\"input\"]=combined_words\n",
    "    zidian[\"output\"]=f\"<im_start> I can extract entities for you, the extracted entities are <<< {result[0][0]} >>>  <im_end>\"\n",
    "    # 直接将label列表合并成一个列表\n",
    "    # combined_labels = [labels for labels in label_lists]\n",
    "    # support_labels.append(combined_labels)\n",
    "    zidian_list.append(zidian)\n",
    "    zidian={}\n",
    "# print(zidian_list)\n",
    "# 将列表转换为JSON格式并保存到文件中\n",
    "with open('data/few-nerd/intra/test_5_1_support.json', 'w', encoding='utf-8') as json_file:\n",
    "    for entry in zidian_list:\n",
    "        json.dump(entry, json_file, ensure_ascii=False)\n",
    "        json_file.write('\\n')\n",
    "\n",
    "print(\"JSON文件已成功保存。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def extract_entities(words, labels):\n",
    "    \"\"\"\n",
    "    根据标签列表中的非 'O' 元素提取对应的实体和标签。\n",
    "\n",
    "    :param words: 单词列表\n",
    "    :param labels: 标签列表\n",
    "    :return: 包含实体和其标签的元组列表\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    current_label = None\n",
    "\n",
    "    for word, label in zip(words, labels):\n",
    "        if label == 'O':\n",
    "            if current_entity:\n",
    "                entities.append((' '.join(current_entity), current_label))\n",
    "                current_entity = []\n",
    "                current_label = None\n",
    "        else:\n",
    "            if current_label == label:\n",
    "                current_entity.append(word)\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities.append((' '.join(current_entity), current_label))\n",
    "                current_entity = [word]\n",
    "                current_label = label\n",
    "\n",
    "    if current_entity:\n",
    "        entities.append((' '.join(current_entity), current_label))\n",
    "\n",
    "    return entities\n",
    "# 加载JSONL文件\n",
    "with open('episode-data/intra/test_5_1.jsonl', 'r') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "# 提取support中的word和label\n",
    "support_words = []\n",
    "support_entity = []\n",
    "support_labels = []\n",
    "zidian={}\n",
    "zidian_list=[]\n",
    "for entry in data:\n",
    "    word_lists = entry['query']['word']\n",
    "    label_lists = entry['query']['label']\n",
    "    for idx in range(len(word_lists)):\n",
    "        result = extract_entities(word_lists[idx], label_lists[idx])\n",
    "    # 将每个word列表合并成一个字符串\n",
    "        combined_words = ' '.join(word_lists[idx])\n",
    "    # support_words.append(combined_words)\n",
    "    zidian[\"instruction\"]=\"Please extract the entity of \"+result[0][1]+\" in the input sentence given below.\"\n",
    "    zidian[\"input\"]=combined_words\n",
    "    zidian[\"output\"]=f\"<im_start> I can extract entities for you, the extracted entities are <<< {result[0][0]} >>>  <im_end>\"\n",
    "    # 直接将label列表合并成一个列表\n",
    "    # combined_labels = [labels for labels in label_lists]\n",
    "    # support_labels.append(combined_labels)\n",
    "    zidian_list.append(zidian)\n",
    "    zidian={}\n",
    "# print(zidian_list)\n",
    "# 将列表转换为JSON格式并保存到文件中\n",
    "with open('data/few-nerd/intra/test_5_1_query.json', 'w', encoding='utf-8') as json_file:\n",
    "    for entry in zidian_list:\n",
    "        json.dump(entry, json_file, ensure_ascii=False)\n",
    "        json_file.write('\\n')\n",
    "\n",
    "print(\"JSON文件已成功保存。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# import glob\n",
    "\n",
    "# 定义要合并的JSON文件路径\n",
    "# file_paths = glob.glob('*.json')  # 这会获取当前目录下所有的.json文件，你可以根据需要调整路径\n",
    "file_paths=['data/few-nerd/intra/train_5_1_support.json','data/few-nerd/intra/train_5_1_query.json','data/few-nerd/intra/dev_5_1_support.json','data/few-nerd/intra/dev_5_1_query.json']\n",
    "\n",
    "# 用于存储合并后的数据\n",
    "merged_data = []\n",
    "\n",
    "# 读取每个JSON文件并合并数据\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            merged_data.append(json.loads(line))\n",
    "\n",
    "# 将合并后的数据写入一个新的JSON文件，每个对象占据文件的一行\n",
    "with open('data/few-nerd/intra/merged_5_1.json', 'w', encoding='utf-8') as output_file:\n",
    "    for entry in merged_data:\n",
    "        json.dump(entry, output_file, ensure_ascii=False)\n",
    "        output_file.write('\\n')\n",
    "\n",
    "print(\"多个JSON文件已成功合并为一个JSON文件。\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Three\tO\n",
    "Russian\tB-MISC\n",
    "servicemen\tO\n",
    "were\tO\n",
    "killed\tO\n",
    "on\tO\n",
    "Saturday\tO\n",
    "when\tO\n",
    "unidentified\tO\n",
    "gunmen\tO\n",
    "attacked\tO\n",
    "guards\tO\n",
    "at\tO\n",
    "an\tO\n",
    "anti-aircraft\tO\n",
    "installation\tO\n",
    "outside\tO\n",
    "Moscow\tB-LOC\n",
    ",\tO\n",
    "Interfax\tB-ORG\n",
    "news\tO\n",
    "agency\tO\n",
    "said\tO\n",
    ".\tO\n",
    "\n",
    "\"\tO\n",
    "I\tO\n",
    "think\tO\n",
    "that\tO\n",
    ",\tO\n",
    "on\tO\n",
    "balance\tO\n",
    ",\tO\n",
    "it\tO\n",
    "is\tO\n",
    "looking\tO\n",
    "a\tO\n",
    "little\tO\n",
    "bit\tO\n",
    "on\tO\n",
    "the\tO\n",
    "strong\tO\n",
    "side\tO\n",
    ",\tO\n",
    "\"\tO\n",
    "Lindsey\tB-PER\n",
    "said\tO\n",
    ".\tO\n",
    "\"\"\"\n",
    "\n",
    "# Mapping from short tags to full descriptions\n",
    "tag_mapping = {\n",
    "    \"B-MISC\": \"miscellaneous\",\n",
    "    \"B-LOC\": \"location\",\n",
    "    \"B-ORG\": \"organization\",\n",
    "    \"B-PER\": \"person\"\n",
    "}\n",
    "\n",
    "# Split the input text into lines and process each line\n",
    "lines = text.strip().split('\\n')\n",
    "\n",
    "# Extract the first column as a sentence\n",
    "sentence = \" \".join([line.split('\\t')[0] for line in lines if '\\t' in line])\n",
    "\n",
    "# Extract words from the first column where the second column is not 'O'\n",
    "non_o_words = [(line.split('\\t')[0], line.split('\\t')[1]) for line in lines if '\\t' in line and line.split('\\t')[1] != 'O']\n",
    "\n",
    "# Format the output\n",
    "extracted_entities = \" \".join([f\"<<< {word}: {tag_mapping[tag]} >>>\" for word, tag in non_o_words])\n",
    "\n",
    "# Create the output dictionary\n",
    "output_dict = {\n",
    "    \"instruction\": \"Please extract the entity in the input sentence given below.\",\n",
    "    \"input\": sentence,\n",
    "    \"output\": f\"<im_start> I can extract entities for you, the extracted entities are {extracted_entities} <im_end>\"\n",
    "}\n",
    "\n",
    "# Print the output dictionary\n",
    "import json\n",
    "print(json.dumps(output_dict, indent=4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "BASKETBALLSOCCER\tO\n",
    "-\tO\n",
    "TROFEJ\tB-MISC\n",
    "BEOGRAD\tI-MISC\n",
    "TOURNAMENT\tI-MISC\n",
    "RESULTS\tO\n",
    ".\tO\n",
    "\n",
    "Atlanta\tB-ORG\n",
    "6\tO\n",
    "CHICAGO\tB-ORG\n",
    "5\tO\n",
    "(\tO\n",
    "2nd\tO\n",
    "game\tO\n",
    ")\tO\n",
    "\n",
    "Eric\tB-PER\n",
    "Anthony\tI-PER\n",
    "hit\tO\n",
    "a\tO\n",
    "pair\tO\n",
    "of\tO\n",
    "solo\tO\n",
    "homers\tO\n",
    "for\tO\n",
    "the\tO\n",
    "Rockies\tB-ORG\n",
    ".\tO\n",
    "\n",
    "2\tO\n",
    "-\tO\n",
    "Monica\tB-PER\n",
    "Seles\tI-PER\n",
    "(\tO\n",
    "U.S.\tB-LOC\n",
    ")\tO\n",
    "beat\tO\n",
    "Dally\tB-PER\n",
    "Randriantefy\tI-PER\n",
    "(\tO\n",
    "Madagascar\tB-LOC\n",
    ")\tO\n",
    "\"\"\"\n",
    "\n",
    "# Mapping from short tags to full descriptions\n",
    "tag_mapping = {\n",
    "    \"B-MISC\": \"miscellaneous\",\n",
    "    \"I-MISC\": \"miscellaneous\",\n",
    "    \"B-LOC\": \"location\",\n",
    "    \"I-LOC\": \"location\",\n",
    "    \"B-ORG\": \"organization\",\n",
    "    \"I-ORG\": \"organization\",\n",
    "    \"B-PER\": \"person\",\n",
    "    \"I-PER\": \"person\"\n",
    "}\n",
    "\n",
    "# Split the input text into lines and process each line\n",
    "lines = text.strip().split('\\n')\n",
    "\n",
    "# Extract the first column as a sentence\n",
    "sentence = \" \".join([line.split('\\t')[0] for line in lines if '\\t' in line])\n",
    "\n",
    "# Extract words from the first column where the second column is not 'O'\n",
    "non_o_words = [(line.split('\\t')[0], line.split('\\t')[1]) for line in lines if '\\t' in line and line.split('\\t')[1] != 'O']\n",
    "\n",
    "# Combine consecutive entities\n",
    "combined_entities = []\n",
    "current_entity = []\n",
    "current_tag = None\n",
    "\n",
    "for word, tag in non_o_words:\n",
    "    full_tag = tag_mapping.get(tag, tag)\n",
    "    if full_tag == current_tag:\n",
    "        current_entity.append(word)\n",
    "    else:\n",
    "        if current_entity:\n",
    "            combined_entities.append((\" \".join(current_entity), current_tag))\n",
    "        current_entity = [word]\n",
    "        current_tag = full_tag\n",
    "\n",
    "if current_entity:\n",
    "    combined_entities.append((\" \".join(current_entity), current_tag))\n",
    "\n",
    "# Format the output\n",
    "extracted_entities = \" \".join([f\"<<< {word}: {tag} >>>\" for word, tag in combined_entities])\n",
    "\n",
    "# Create the output dictionary\n",
    "output_dict = {\n",
    "    \"instruction\": \"Please extract the entity in the input sentence given below.\",\n",
    "    \"input\": sentence,\n",
    "    \"output\": f\"<im_start> I can extract entities for you, the extracted entities are {extracted_entities} <im_end>\"\n",
    "}\n",
    "\n",
    "# Print the output dictionary\n",
    "import json\n",
    "print(json.dumps(output_dict, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Mapping from short tags to full descriptions\n",
    "tag_mapping = {\n",
    "    \"B-MISC\": \"miscellaneous\",\n",
    "    \"I-MISC\": \"miscellaneous\",\n",
    "    \"B-LOC\": \"location\",\n",
    "    \"I-LOC\": \"location\",\n",
    "    \"B-ORG\": \"organization\",\n",
    "    \"I-ORG\": \"organization\",\n",
    "    \"B-PER\": \"person\",\n",
    "    \"I-PER\": \"person\"\n",
    "}\n",
    "\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Split the input text into lines and process each line\n",
    "    lines = text.strip().split('\\n')\n",
    "\n",
    "    # Extract the first column as a sentence\n",
    "    sentence = \" \".join([line.split('\\t')[0] for line in lines if '\\t' in line])\n",
    "\n",
    "    # Extract words from the first column where the second column is not 'O'\n",
    "    non_o_words = [(line.split('\\t')[0], line.split('\\t')[1]) for line in lines if '\\t' in line and line.split('\\t')[1] != 'O']\n",
    "\n",
    "    # Combine consecutive entities\n",
    "    combined_entities = []\n",
    "    current_entity = []\n",
    "    current_tag = None\n",
    "\n",
    "    for word, tag in non_o_words:\n",
    "        full_tag = tag_mapping.get(tag, tag)\n",
    "        if full_tag == current_tag:\n",
    "            current_entity.append(word)\n",
    "        else:\n",
    "            if current_entity:\n",
    "                combined_entities.append((\" \".join(current_entity), current_tag))\n",
    "            current_entity = [word]\n",
    "            current_tag = full_tag\n",
    "\n",
    "    if current_entity:\n",
    "        combined_entities.append((\" \".join(current_entity), current_tag))\n",
    "\n",
    "    # Format the output\n",
    "    extracted_entities = \" \".join([f\"<<< {word}: {tag} >>>\" for word, tag in combined_entities])\n",
    "\n",
    "    # Create the output dictionary\n",
    "    output_dict = {\n",
    "        \"instruction\": \"Please extract the entity in the input sentence given below.\",\n",
    "        \"input\": sentence,\n",
    "        \"output\": f\"<im_start> I can extract entities for you, the extracted entities are {extracted_entities} <im_end>\"\n",
    "    }\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    # Get all txt files in the folder\n",
    "    txt_files = glob.glob(os.path.join(folder_path, '*.txt'))\n",
    "    \n",
    "    for file_path in txt_files:\n",
    "        output_dict = process_file(file_path)\n",
    "        # Save the result to a new file\n",
    "        output_file_path = file_path.replace('.txt', '_processed.json')\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            json.dump(output_dict, output_file, indent=4)\n",
    "\n",
    "# Example usage\n",
    "folder_path = '/data/guoquanjiang/TadNER/data_raw/CONLL2003/samples-CONLL2003-1shot'  # Replace with the path to your folder containing txt files\n",
    "process_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Mapping from short tags to full descriptions\n",
    "tag_mapping = {\n",
    "    \"B-MISC\": \"miscellaneous\",\n",
    "    \"I-MISC\": \"miscellaneous\",\n",
    "    \"B-LOC\": \"location\",\n",
    "    \"I-LOC\": \"location\",\n",
    "    \"B-ORG\": \"organization\",\n",
    "    \"I-ORG\": \"organization\",\n",
    "    \"B-PER\": \"person\",\n",
    "    \"I-PER\": \"person\"\n",
    "}\n",
    "\n",
    "def process_sentence(lines):\n",
    "    # Extract the first column as a sentence\n",
    "    sentence = \" \".join([line.split('\\t')[0] for line in lines if '\\t' in line])\n",
    "\n",
    "    # Extract words from the first column where the second column is not 'O'\n",
    "    non_o_words = [(line.split('\\t')[0], line.split('\\t')[1]) for line in lines if '\\t' in line and line.split('\\t')[1] != 'O']\n",
    "\n",
    "    # Combine consecutive entities\n",
    "    combined_entities = []\n",
    "    current_entity = []\n",
    "    current_tag = None\n",
    "\n",
    "    for word, tag in non_o_words:\n",
    "        full_tag = tag_mapping.get(tag, tag)\n",
    "        if full_tag == current_tag:\n",
    "            current_entity.append(word)\n",
    "        else:\n",
    "            if current_entity:\n",
    "                combined_entities.append((\" \".join(current_entity), current_tag))\n",
    "            current_entity = [word]\n",
    "            current_tag = full_tag\n",
    "\n",
    "    if current_entity:\n",
    "        combined_entities.append((\" \".join(current_entity), current_tag))\n",
    "\n",
    "    # Format the output\n",
    "    extracted_entities = \" \".join([f\"<<< {word}: {tag} >>>\" for word, tag in combined_entities])\n",
    "\n",
    "    # Create the output dictionary\n",
    "    output_dict = {\n",
    "        \"instruction\": \"Please extract the entity in the input sentence given below.\",\n",
    "        \"input\": sentence,\n",
    "        \"output\": f\"<im_start> I can extract entities for you, the extracted entities are {extracted_entities} <im_end>\"\n",
    "    }\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Split the input text into sentences based on empty lines\n",
    "    sentences = text.strip().split('\\n\\n')\n",
    "    \n",
    "    output_dicts = []\n",
    "    for sentence in sentences:\n",
    "        lines = sentence.strip().split('\\n')\n",
    "        output_dict = process_sentence(lines)\n",
    "        output_dicts.append(output_dict)\n",
    "    \n",
    "    return output_dicts\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    # Get all txt files in the folder\n",
    "    txt_files = glob.glob(os.path.join(folder_path, '*.txt'))\n",
    "    \n",
    "    for file_path in txt_files:\n",
    "        output_dicts = process_file(file_path)\n",
    "        # Save the result to a new file\n",
    "        output_file_path = file_path.replace('.txt', '_processed.json')\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            json.dump(output_dicts, output_file, indent=4)\n",
    "\n",
    "# Example usage\n",
    "folder_path = '/data/guoquanjiang/CONTaiNER/data/domain-transfer-support-sets/samples-conll-1shot'  # Replace with the path to your folder containing txt files\n",
    "process_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Mapping from short tags to full descriptions\n",
    "tag_mapping = {\n",
    "    \"B-MISC\": \"miscellaneous\",\n",
    "    \"I-MISC\": \"miscellaneous\",\n",
    "    \"B-LOC\": \"location\",\n",
    "    \"I-LOC\": \"location\",\n",
    "    \"B-ORG\": \"organization\",\n",
    "    \"I-ORG\": \"organization\",\n",
    "    \"B-PER\": \"person\",\n",
    "    \"I-PER\": \"person\"\n",
    "}\n",
    "\n",
    "def process_sentence(lines):\n",
    "    # Extract the first column as a sentence\n",
    "    sentence = \" \".join([line.split('\\t')[0] for line in lines if '\\t' in line])\n",
    "\n",
    "    # Extract words from the first column where the second column is not 'O'\n",
    "    non_o_words = [(line.split('\\t')[0], line.split('\\t')[1]) for line in lines if '\\t' in line and line.split('\\t')[1] != 'O']\n",
    "\n",
    "    # Combine consecutive entities\n",
    "    combined_entities = []\n",
    "    current_entity = []\n",
    "    current_tag = None\n",
    "\n",
    "    for word, tag in non_o_words:\n",
    "        full_tag = tag_mapping.get(tag, tag)\n",
    "        if current_tag is None:\n",
    "            current_entity.append(word)\n",
    "            current_tag = full_tag\n",
    "        elif full_tag == current_tag and tag.startswith('I'):\n",
    "            current_entity.append(word)\n",
    "        else:\n",
    "            if current_entity:\n",
    "                combined_entities.append((\" \".join(current_entity), current_tag))\n",
    "            current_entity = [word]\n",
    "            current_tag = full_tag\n",
    "\n",
    "    if current_entity:\n",
    "        combined_entities.append((\" \".join(current_entity), current_tag))\n",
    "\n",
    "    # Format the output\n",
    "    extracted_entities = \" \".join([f\"<<< {word}: {tag} >>>\" for word, tag in combined_entities])\n",
    "\n",
    "    # Create the output dictionary\n",
    "    output_dict = {\n",
    "        \"instruction\": \"Please extract the entity in the input sentence given below.\",\n",
    "        \"input\": sentence,\n",
    "        \"output\": f\"<im_start> I can extract entities for you, the extracted entities are {extracted_entities} <im_end>\"\n",
    "    }\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Split the input text into sentences based on empty lines\n",
    "    sentences = text.strip().split('\\n\\n')\n",
    "    \n",
    "    output_dicts = []\n",
    "    for sentence in sentences:\n",
    "        lines = sentence.strip().split('\\n')\n",
    "        output_dict = process_sentence(lines)\n",
    "        output_dicts.append(output_dict)\n",
    "    \n",
    "    return output_dicts\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    # Get all txt files in the folder\n",
    "    txt_files = glob.glob(os.path.join(folder_path, '*.txt'))\n",
    "    \n",
    "    for file_path in txt_files:\n",
    "        output_dicts = process_file(file_path)\n",
    "        # Save the result to a new file\n",
    "        output_file_path = file_path.replace('.txt', '_processed.json')\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            json.dump(output_dicts, output_file, indent=4)\n",
    "\n",
    "# Example usage\n",
    "folder_path = '/data/guoquanjiang/CONTaiNER/data/domain-transfer-support-sets/samples-conll-1shot'  # Replace with the path to your folder containing txt files\n",
    "process_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Mapping from short tags to full descriptions\n",
    "tag_mapping = {\n",
    "    \"B-MISC\": \"miscellaneous\",\n",
    "    \"I-MISC\": \"miscellaneous\",\n",
    "    \"B-LOC\": \"location\",\n",
    "    \"I-LOC\": \"location\",\n",
    "    \"B-ORG\": \"organization\",\n",
    "    \"I-ORG\": \"organization\",\n",
    "    \"B-PER\": \"person\",\n",
    "    \"I-PER\": \"person\",\n",
    "    \"O\": \"O\"\n",
    "}\n",
    "\n",
    "def process_sentence(lines):\n",
    "    # Extract the first column as a sentence\n",
    "    sentence = \" \".join([line.split()[0] for line in lines if len(line.split()) == 4])\n",
    "\n",
    "    # Extract words from the first column where the fourth column is not 'O'\n",
    "    non_o_words = [(line.split()[0], line.split()[3]) for line in lines if len(line.split()) == 4 and line.split()[3] != 'O']\n",
    "\n",
    "    # Combine consecutive entities\n",
    "    combined_entities = []\n",
    "    current_entity = []\n",
    "    current_tag = None\n",
    "\n",
    "    for word, tag in non_o_words:\n",
    "        full_tag = tag_mapping.get(tag, tag)\n",
    "        if current_tag is None:\n",
    "            current_entity.append(word)\n",
    "            current_tag = full_tag\n",
    "        elif full_tag == current_tag and tag.startswith('I'):\n",
    "            current_entity.append(word)\n",
    "        else:\n",
    "            if current_entity:\n",
    "                combined_entities.append((\" \".join(current_entity), current_tag))\n",
    "            current_entity = [word]\n",
    "            current_tag = full_tag\n",
    "\n",
    "    if current_entity:\n",
    "        combined_entities.append((\" \".join(current_entity), current_tag))\n",
    "\n",
    "    # Format the output\n",
    "    extracted_entities = \" \".join([f\"<<< {word}: {tag} >>>\" for word, tag in combined_entities if tag != \"O\"])\n",
    "\n",
    "    # Create the output dictionary\n",
    "    output_dict = {\n",
    "        \"instruction\": \"Please extract the entity in the input sentence given below.\",\n",
    "        \"input\": sentence,\n",
    "        \"output\": f\"<im_start> I can extract entities for you, the extracted entities are {extracted_entities} <im_end>\"\n",
    "    }\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Split the input text into sentences based on empty lines\n",
    "    sentences = text.strip().split('\\n\\n')\n",
    "    \n",
    "    output_dicts = []\n",
    "    for sentence in sentences:\n",
    "        lines = sentence.strip().split('\\n')\n",
    "        output_dict = process_sentence(lines)\n",
    "        output_dicts.append(output_dict)\n",
    "    \n",
    "    return output_dicts\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    # Get all txt files in the folder\n",
    "    txt_files = glob.glob(os.path.join(folder_path, '*.txt'))\n",
    "    \n",
    "    for file_path in txt_files:\n",
    "        output_dicts = process_file(file_path)\n",
    "        # Save the result to a new file\n",
    "        output_file_path = file_path.replace('.txt', '_processed.json')\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            json.dump(output_dicts, output_file, indent=4)\n",
    "\n",
    "# Example usage\n",
    "folder_path = '/data/guoquanjiang/TadNER/data_raw/CONLL2003'  # Replace with the path to your folder containing txt files\n",
    "process_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Mapping from short tags to full descriptions\n",
    "tag_mapping = {\n",
    "    \"B-MISC\": \"miscellaneous\",\n",
    "    \"I-MISC\": \"miscellaneous\",\n",
    "    \"B-LOC\": \"location\",\n",
    "    \"I-LOC\": \"location\",\n",
    "    \"B-ORG\": \"organization\",\n",
    "    \"I-ORG\": \"organization\",\n",
    "    \"B-PER\": \"person\",\n",
    "    \"I-PER\": \"person\",\n",
    "    \"O\": \"O\"\n",
    "}\n",
    "\n",
    "def process_sentence(lines):\n",
    "    # Extract the first column as a sentence\n",
    "    sentence = \" \".join([line.split()[0] for line in lines if len(line.split()) == 4])\n",
    "\n",
    "    # Extract words from the first column where the fourth column is not 'O'\n",
    "    non_o_words = [(line.split()[0], line.split()[3]) for line in lines if len(line.split()) == 4 and line.split()[3] != 'O']\n",
    "\n",
    "    # Combine consecutive entities\n",
    "    combined_entities = []\n",
    "    current_entity = []\n",
    "    current_tag = None\n",
    "\n",
    "    for word, tag in non_o_words:\n",
    "        full_tag = tag_mapping.get(tag, tag)\n",
    "        if current_tag is None:\n",
    "            current_entity.append(word)\n",
    "            current_tag = full_tag\n",
    "        elif full_tag == current_tag and tag.startswith('I'):\n",
    "            current_entity.append(word)\n",
    "        else:\n",
    "            if current_entity:\n",
    "                combined_entities.append((\" \".join(current_entity), current_tag))\n",
    "            current_entity = [word]\n",
    "            current_tag = full_tag\n",
    "\n",
    "    if current_entity:\n",
    "        combined_entities.append((\" \".join(current_entity), current_tag))\n",
    "\n",
    "    # Format the output\n",
    "    extracted_entities = \" \".join([f\"<<< {word}: {tag} >>>\" for word, tag in combined_entities if tag != \"O\"])\n",
    "\n",
    "    if not extracted_entities.strip():\n",
    "        return None  # Skip if no entities found\n",
    "\n",
    "    # Create the output dictionary\n",
    "    output_dict = {\n",
    "        \"instruction\": \"Please extract the entity in the input sentence given below.\",\n",
    "        \"input\": sentence,\n",
    "        \"output\": f\"<im_start> I can extract entities for you, the extracted entities are {extracted_entities} <im_end>\"\n",
    "    }\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Split the input text into sentences based on empty lines\n",
    "    sentences = text.strip().split('\\n\\n')\n",
    "    \n",
    "    output_dicts = []\n",
    "    for sentence in sentences:\n",
    "        lines = sentence.strip().split('\\n')\n",
    "        output_dict = process_sentence(lines)\n",
    "        if output_dict:  # Only add non-empty dictionaries\n",
    "            output_dicts.append(output_dict)\n",
    "    \n",
    "    return output_dicts\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    # Get all txt files in the folder\n",
    "    txt_files = glob.glob(os.path.join(folder_path, '*.txt'))\n",
    "    \n",
    "    for file_path in txt_files:\n",
    "        output_dicts = process_file(file_path)\n",
    "        if output_dicts:  # Only save if there are valid dictionaries\n",
    "            # Save the result to a new file\n",
    "            output_file_path = file_path.replace('.txt', '_processed.json')\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                json.dump(output_dicts, output_file, indent=4)\n",
    "\n",
    "# Example usage\n",
    "folder_path = '/data/guoquanjiang/TadNER/data_raw/CONLL2003'  # Replace with the path to your folder containing txt files\n",
    "process_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Mapping from short tags to full descriptions\n",
    "tag_mapping = {\n",
    "    \"B-MISC\": \"miscellaneous\",\n",
    "    \"I-MISC\": \"miscellaneous\",\n",
    "    \"B-LOC\": \"location\",\n",
    "    \"I-LOC\": \"location\",\n",
    "    \"B-ORG\": \"organization\",\n",
    "    \"I-ORG\": \"organization\",\n",
    "    \"B-PER\": \"person\",\n",
    "    \"I-PER\": \"person\",\n",
    "    \"O\": \"O\"\n",
    "}\n",
    "\n",
    "def process_sentence(lines):\n",
    "    # Extract the first column as a sentence\n",
    "    sentence = \" \".join([line.split()[0] for line in lines if len(line.split()) == 4])\n",
    "\n",
    "    # Extract words from the first column where the fourth column is not 'O'\n",
    "    non_o_words = [(line.split()[0], line.split()[3]) for line in lines if len(line.split()) == 4 and line.split()[3] != 'O']\n",
    "\n",
    "    # Combine consecutive entities\n",
    "    combined_entities = []\n",
    "    current_entity = []\n",
    "    current_tag = None\n",
    "\n",
    "    for word, tag in non_o_words:\n",
    "        full_tag = tag_mapping.get(tag, tag)\n",
    "        if current_tag is None:\n",
    "            current_entity.append(word)\n",
    "            current_tag = full_tag\n",
    "        elif full_tag == current_tag and tag.startswith('I'):\n",
    "            current_entity.append(word)\n",
    "        else:\n",
    "            if current_entity:\n",
    "                combined_entities.append((\" \".join(current_entity), current_tag))\n",
    "            current_entity = [word]\n",
    "            current_tag = full_tag\n",
    "\n",
    "    if current_entity:\n",
    "        combined_entities.append((\" \".join(current_entity), current_tag))\n",
    "\n",
    "    # Format the output\n",
    "    extracted_entities = \" \".join([f\"<<< {word}: {tag} >>>\" for word, tag in combined_entities if tag != \"O\"])\n",
    "\n",
    "    if not extracted_entities.strip():\n",
    "        return None  # Skip if no entities found\n",
    "\n",
    "    # Create the output dictionary\n",
    "    output_dict = {\n",
    "        \"instruction\": \"Please extract the entity in the input sentence given below.\",\n",
    "        \"input\": sentence,\n",
    "        \"output\": f\"<im_start> I can extract entities for you, the extracted entities are {extracted_entities} <im_end>\"\n",
    "    }\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Split the input text into sentences based on empty lines\n",
    "    sentences = text.strip().split('\\n\\n')\n",
    "    \n",
    "    output_dicts = []\n",
    "    for sentence in sentences:\n",
    "        lines = sentence.strip().split('\\n')\n",
    "        output_dict = process_sentence(lines)\n",
    "        if output_dict:  # Only add non-empty dictionaries\n",
    "            output_dicts.append(output_dict)\n",
    "    \n",
    "    return output_dicts\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    # Get all txt files in the folder\n",
    "    txt_files = glob.glob(os.path.join(folder_path, '*.txt'))\n",
    "    \n",
    "    for file_path in txt_files:\n",
    "        output_dicts = process_file(file_path)\n",
    "        if output_dicts:  # Only save if there are valid dictionaries\n",
    "            # Save the result to a new file\n",
    "            output_file_path = file_path.replace('.txt', '_processed.json')\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                for output_dict in output_dicts:\n",
    "                    json.dump(output_dict, output_file)\n",
    "                    output_file.write(\"\\n\")  # Write each dictionary on a new line\n",
    "\n",
    "# Example usage\n",
    "folder_path = '/data/guoquanjiang/TadNER/data_raw/CONLL2003'  # Replace with the path to your folder containing txt files\n",
    "process_folder(folder_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
